{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8031f0a",
   "metadata": {},
   "source": [
    "# Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce0369",
   "metadata": {},
   "source": [
    "## Why GNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a542249",
   "metadata": {},
   "source": [
    "Answer briefly:\n",
    "\n",
    "1. Consider the given list of limitations of shallow encoders like random walk based node embedding methods. How do GNNs solve them?\n",
    "    - Poor scalability ($|V|d$ parameters are needed, $d$: dimension of the embedding space)\n",
    "    - Transductive nature (Cannot obtain embeddings for nodes not in the training set.)\n",
    "    - Cannot capture structural similarity\n",
    "    - Cannot utilize node, edge, and graph features\n",
    "\n",
    "1. Given an undirected graph $G=(V,E)$, let's flatten the adjacency matrix $A$ (i.e., concatenate the rows into a single vector) and feed it to a Multi-Layer Perceptron (MLP). What's wrong with this approach?\n",
    "\n",
    "1. In a CNN, a convolutional kernel aggregates information from local pixel neighborhoods. What would \"locality\" mean on a graph? How could we define a convolution operation that respects this locality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc1a904",
   "metadata": {},
   "source": [
    "## Permutation Invariance and Equivariance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90362a31",
   "metadata": {},
   "source": [
    "1. Consider a graph with three node labels A, B, C and the adjacency matrix:\n",
    "    $$\n",
    "        A = \\begin{bmatrix}\n",
    "        0 & 1 & 1\\\\\n",
    "        1 & 0 & 0\\\\\n",
    "        1 & 0 & 0\n",
    "        \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "    Suppose we feed this graph into a GNN layer defined as:\n",
    "    $$\n",
    "        h_v^{(1)} = \\sigma (W \\cdot \\text{AGG}(\\{h_u^{(0)}: u \\in N(v)\\}))\n",
    "    $$\n",
    "\n",
    "    where AGG is the **sum** function.\n",
    "\n",
    "    If we permute the node order to (C, A, B), will the node embeddings change after the layer? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6033e3",
   "metadata": {},
   "source": [
    "2. Let $A \\in \\{0,1\\}^{n \\times n}$ be the adjacency matrix and $X \\in \\mathbb{R}^{n \\times d}$ be the node feature matrix. Let $P$ be an $n \\times n$ permutation matrix (it reorders node indices). Permutation of the graph means:\n",
    "    $$\n",
    "        A^\\prime = PAP^\\intercal, \\quad X^\\prime=PX\n",
    "    $$\n",
    "\n",
    "    For each function $f(A,X)$ below, determine wheteher it is:\n",
    "    - Permutation invariant: $f(A^\\prime ,X^\\prime) = f(A,X)$\n",
    "    - Permutation equivariant: $f(A^\\prime ,X^\\prime) = Pf(A,X)$\n",
    "    - Neither\n",
    "\n",
    "    | Function                             | Inv./Equiv./Neither |\n",
    "    |--------------------------------------|---------------------|\n",
    "    | $f(A,X) = 1^\\intercal X$             |                     |\n",
    "    | $f(A,X) = X$                         |                     |\n",
    "    | $f(A,X) = AX$                        |                     |\n",
    "    | $f(A,X) = A^\\intercal X$             |                     |\n",
    "    | $f(A,X) = \\text{ReLU}(AXW)$          |                     |\n",
    "    | $f(A,X) = \\frac{1}{n} 1^\\intercal X$ |                     |\n",
    "    | $f(A,X) = X^\\intercal X$             |                     |\n",
    "    | $f(A,X) = XW$                        |                     |\n",
    "    | $f(A,X) = A_{1,:}X$                  |                     |\n",
    "    | $f(A,X) = \\text{sort}(X)$            |                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354c75a",
   "metadata": {},
   "source": [
    "## One GNN Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da1390",
   "metadata": {},
   "source": [
    "Consider the following simple undirected graph $G=(V,E)$:\n",
    "$$\n",
    "    V=\\{ 1,2,3 \\}, \\quad E=\\{ \\{1,2\\},\\{2,3\\} \\}\n",
    "$$\n",
    "\n",
    "The initial node feature matrix and the (unweighted) adjacency matrix are given as follows:\n",
    "$$\n",
    "    X = \n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        0 & 1 \\\\\n",
    "        1 & 1 \n",
    "    \\end{bmatrix}, \\quad\n",
    "    A = \n",
    "    \\begin{bmatrix}\n",
    "        0 & 1 & 0 \\\\\n",
    "        1 & 0 & 1 \\\\\n",
    "        0 & 1 & 0 \n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We apply one Graph Convolutional Network (GCN) layer as defined by Kipf & Welling (2017):\n",
    "$$\n",
    "    H = \\sigma(\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}XW)\n",
    "$$\n",
    "where \n",
    "\n",
    "- $D$ is the degree matrix ($D=\\text{diag}(1,2,1)$)\n",
    "- $\\tilde{A}=A+I$\n",
    "- $\\tilde{D}_{ii}=\\sum_j \\tilde{A}_{ij}$\n",
    "- $\\sigma(\\cdot)$ is ReLU\n",
    "\n",
    "and for simplicity, the weight matrix is \n",
    "$$\n",
    "    W = \n",
    "    \\begin{bmatrix}\n",
    "        1 & 0 \\\\\n",
    "        0 & 1 \\\\\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Compute $\\tilde{A}$ and $\\tilde{D}$\n",
    "1. Compute the normalized adjacency $\\tilde{D}^{-1/2}\\tilde{A}\\tilde{D}^{-1/2}$\n",
    "1. Multiply with $XW$\n",
    "1. Apply the ReLU\n",
    "1. Write down the resulting node embeddings $H_1,H_2,H_3$\n",
    "1. Now compare the initial node features $X$ and the embeddings $H$. How did each node's location in the embedding space change and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512ed7fc",
   "metadata": {},
   "source": [
    "## Programming: One GNN Layer with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15996701",
   "metadata": {},
   "source": [
    "In this exercise, you'll implement the same task given in the previous example with `torch`. \n",
    "\n",
    "1. Complete the following code snippet and reproduce your result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64772a19",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial features X:\n",
      " tensor([[1., 0.],\n",
      "        [0., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "Normalized adjacency A_norm:\n",
      " 0\n",
      "\n",
      "Embeddings H:\n",
      " 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# initial features\n",
    "X = torch.tensor([\n",
    "    [1., 0.],  # Node 1\n",
    "    [0., 1.],  # Node 2\n",
    "    [1., 1.]   # Node 3\n",
    "])\n",
    "\n",
    "# toy graph: 1–2–3\n",
    "A = torch.tensor([\n",
    "    [0., 1., 0.],\n",
    "    [1., 0., 1.],\n",
    "    [0., 1., 0.]\n",
    "])\n",
    "\n",
    "# weight matrix (identity for simplicity)\n",
    "W = torch.eye(2)\n",
    "\n",
    "# Task 1: Compute A_hat and D_hat\n",
    "A_hat = 0\n",
    "D_hat = 0\n",
    "\n",
    "# Task 2: Compute normalized adjacency\n",
    "A_norm = 0\n",
    "\n",
    "# Task 3-4: Multiply normalized adjacency with XW and apply ReLU\n",
    "H = 0\n",
    "\n",
    "print(\"Initial features X:\\n\", X)\n",
    "print(\"\\nNormalized adjacency A_norm:\\n\", A_norm)\n",
    "print(\"\\nEmbeddings H:\\n\", H)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc60d72c",
   "metadata": {},
   "source": [
    "2. Now, re-apply the GCN layer 10 more times. What happens to the embeddings? Interpret your results.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
