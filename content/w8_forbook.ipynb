{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcdba7c",
   "metadata": {},
   "source": [
    "# Graph Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58e81d6",
   "metadata": {},
   "source": [
    "## Self-Attention as Message Passing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf1930",
   "metadata": {},
   "source": [
    "Let's assume that we have three tokens with scalar features:\n",
    "$$x_1 = 1, \\quad x_2 = 2, \\quad x_3 = 0$$\n",
    "\n",
    "Note: This is a toy model. In practice, token features are high-dimensional vectors.\n",
    "\n",
    "For simplicity, we define $W^Q=W^K=W^V=I$, therefore:\n",
    "$$q_i = x_i,\\quad k_i = x_i,\\quad v_i = x_i$$\n",
    "\n",
    "And the self-attention update for token 1:\n",
    "$$z_1 = \\sum_{j=1}^3 \\alpha_{1j} v_j, \\quad \\alpha_{1j} = \\frac{e^{q_1 k_j}}{\\sum_{\\ell} e^{q_1 k_\\ell}}$$\n",
    "\n",
    "Before doing any math, answer:\n",
    "\n",
    "1. Which token do you expect token 1 to pay most attention to?\n",
    "2. Which token should receive the least attention from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b157d18",
   "metadata": {},
   "source": [
    "Now, compute and answer:\n",
    "\n",
    "3. All $\\alpha_{1j}$ and then $z_1$. Describe what $z_1$ represents in this example.\n",
    "4. Are $\\alpha_{12}$ and $\\alpha_{21}$ equal? What does this say about attention as an \"edge weight\"?\n",
    "5. What happens if all $x_i$ are equal? What kind of GNN is this equivalent to?\n",
    "6. What changes in the self-attention mechanism when moving from a Transformer to a GAT?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025af746",
   "metadata": {},
   "source": [
    "## Graph Laplacian Magic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241db39b",
   "metadata": {},
   "source": [
    "In previous lectures, we used the Laplacian matrix $L$, but what does it actually mean?\n",
    "\n",
    "Given an undirected graph with adjacency matrix $A$ and degree matrix $D$, $L$ is defined as:\n",
    "$$L = D - A$$\n",
    "\n",
    "Consider the following path graph of $3$ nodes:\n",
    "\n",
    "![alt text](assets/w8_path.png \"G\")\n",
    "\n",
    "1. What is the Laplacian matrix for this graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c550d2",
   "metadata": {},
   "source": [
    "2. Verify that the following are eigenvectors of $L$, and find the corresponding eigenvalues:\n",
    "    - $(-1,0,1)^\\top$\n",
    "    - $(1,1,1)^\\top$\n",
    "    - $(1,-2,1)^\\top$\n",
    "\n",
    "    Refresher: $Lv=\\lambda v$, where $v$ is the eigenvector and $\\lambda$ is the eigenvalue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17aa3f1",
   "metadata": {},
   "source": [
    "3. Order the eigenvectors from slowly varying to rapidly varying (i.e., smooth vs oscillatory) across the graph. Then, look at the corresponding eigenvalues. What do you observe about how the eigenvalues are ordered relative to the variation speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3c144",
   "metadata": {},
   "source": [
    "4. Based on your finding in (3), which eigenvectors capture the global structure and which eigenvectors capture the local variation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d5b8f7",
   "metadata": {},
   "source": [
    "5. Now let's verify our findings in a relatively larger graph. We will revisit the Karate Club graph and plot what the eigenvectors highlight within the graph.\n",
    "\n",
    "    The following code snippet computes the eigenvectors and sorts them based on the corresponding eigenvalues. Plot different eigenvectors across the graph to see which information they encode (local vs global). Does it match with your finding in (4)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f866aa",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the Karate Club graph\n",
    "G = nx.karate_club_graph()\n",
    "\n",
    "# compute the Laplacian matrix\n",
    "L = nx.laplacian_matrix(G).toarray()\n",
    "\n",
    "# compute eigenvalues and eigenvectors\n",
    "eigvals, eigvecs = np.linalg.eigh(L)\n",
    "\n",
    "# sort eigenvalues and eigenvectors\n",
    "idx = np.argsort(eigvals)\n",
    "eigvals = eigvals[idx]\n",
    "eigvecs = eigvecs[:, idx]\n",
    "\n",
    "# plot the eigenvectors\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "for k in [1]:\n",
    "\tplt.figure()\n",
    "\tnx.draw(G, pos, node_color=eigvecs[:, k], cmap='coolwarm', with_labels=False)\n",
    "\tplt.title(f\"Laplacian eigenvector {k}\")\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcc5c9",
   "metadata": {},
   "source": [
    "## Eigenvector Sign Ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35207fe0",
   "metadata": {},
   "source": [
    "Recall that $Lv=\\lambda v$, where $v$ is the eigenvector and $\\lambda$ is the eigenvalue. This also means:\n",
    "$$L(-v)=\\lambda (-v)$$\n",
    "\n",
    "1. Why is this a problem when we want to use the eigenvectors as positional encodings?\n",
    "\n",
    "2. Would an attention-based model automatically be invariant to this sign ambiguity? Why or why not?\n",
    "\n",
    "3. If we need sign invariance, why don't we just take $|v|$ before using it as a positional encoding?\n",
    "\n",
    "4. How does `SignNet` ([paper link](https://arxiv.org/pdf/2202.13013)) solve this problem? You can play with the following code snippet which implements a simplifed version of `SignNet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092106b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleSignNet(nn.Module):\n",
    "    def __init__(self, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.phi = nn.Sequential(\n",
    "            nn.Linear(1, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, v):\n",
    "        # v: (n,) eigenvector\n",
    "        v = v.unsqueeze(-1)        # (n, 1)\n",
    "\n",
    "        out_pos = self.phi(v)      # φ(v)\n",
    "        out_neg = self.phi(-v)     # φ(-v)\n",
    "\n",
    "        return out_pos + out_neg   # sign-invariant\n",
    "    \n",
    "# toy eigenvector\n",
    "v = torch.tensor([1.0, -2.0, 0.5])\n",
    "\n",
    "model = SimpleSignNet()\n",
    "\n",
    "z1 = model(v)\n",
    "z2 = model(-v)\n",
    "\n",
    "print(torch.allclose(z1, z2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
