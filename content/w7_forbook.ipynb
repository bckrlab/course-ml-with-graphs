{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e1d69d",
   "metadata": {},
   "source": [
    "# Theory of GNNs 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867afedf",
   "metadata": {},
   "source": [
    "## Counting Cycle Lengths with GNNs\n",
    "\n",
    "In the lecture, we listed the following observaitons about a perfect GNN:\n",
    "\n",
    "- **Observation 1:** If two nodes have the **same** neighborhood structure, they must have the **same** embedding.\n",
    "- **Observation 2:** If two nodes have **different** neighborhood structure, they must have **different** embeddings.\n",
    "\n",
    "Considering observation 2, we stated that message-passing GNNs cannot count the cycle length, and we showed the following example with cycle lengths of 3 and 4.\n",
    "\n",
    "![alt text](assets/w7_cycles.png \"G\")\n",
    "\n",
    "1. What's the reason that a message-passing based GNN cannot distinguish $C3$ and $C4$? Is this true for any $Ci$ and $Cj$ where $i,j \\geq 3$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3686823",
   "metadata": {},
   "source": [
    "2. Let's verify this using `PyG`. The following code snippet involves a simple GNN with GCN layers and a global pooling to get graph-level representations. Play with the parameters (e.g., number of layers, dimensions) to check if our statement holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f5819df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: tensor([[0.5712, 0.3294, 0.1356, 0.2964, 0.0000, 0.0000, 0.4536, 0.0512]])\n",
      "C4: tensor([[0.5712, 0.3294, 0.1356, 0.2964, 0.0000, 0.0000, 0.4536, 0.0512]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.pool import global_mean_pool\n",
    "\n",
    "class SimpleGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return global_mean_pool(x, batch)\n",
    "\n",
    "# C3\n",
    "edge_index_C3 = torch.tensor([\n",
    "    [0,1, 1,2, 2,0],\n",
    "    [1,0, 2,1, 0,2]\n",
    "], dtype=torch.long)\n",
    "x_C3 = torch.ones((3,1))\n",
    "data_C3 = Data(x=x_C3, edge_index=edge_index_C3)\n",
    "\n",
    "# C4\n",
    "edge_index_C4 = torch.tensor([\n",
    "    [0,1, 1,2, 2,3, 3,0],\n",
    "    [1,0, 2,1, 3,2, 0,3]\n",
    "], dtype=torch.long)\n",
    "x_C4 = torch.ones((4,1))\n",
    "data_C4 = Data(x=x_C4, edge_index=edge_index_C4)\n",
    "\n",
    "# run the model\n",
    "model = SimpleGCN()\n",
    "with torch.no_grad():\n",
    "    out_C3_mean = model(data_C3.x, data_C3.edge_index, data_C3.batch)\n",
    "    out_C4_mean = model(data_C4.x, data_C4.edge_index, data_C4.batch)\n",
    "\n",
    "print(\"C3:\", out_C3_mean)\n",
    "print(\"C4:\", out_C4_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241c60d0",
   "metadata": {},
   "source": [
    "3. What if we used global sum pooling instead of mean? Edit the code snippet and test if the embeddings differ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad7041f",
   "metadata": {},
   "source": [
    "4. What about the following graphs? Can sum pooling distinguish them? Run the following code snippet and play with parameters to formulate your answer.\n",
    "\n",
    "![alt text](assets/w7_cycles_2.png \"G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af949fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C6: tensor([[0.0100, 0.0000, 0.4739, 0.0000, 1.2981, 1.0449, 0.7568, 0.0000]])\n",
      "C3C3: tensor([[0.0100, 0.0000, 0.4739, 0.0000, 1.2981, 1.0449, 0.7568, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn.pool import global_add_pool\n",
    "\n",
    "class SimpleGCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(1, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        return global_add_pool(x, batch)\n",
    "\n",
    "# C6\n",
    "edge_index_C6 = torch.tensor([\n",
    "    [0, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 0],\n",
    "    [1, 2, 3, 4, 5, 0, 0, 1, 2, 3, 4, 5]\n",
    "], dtype=torch.long)\n",
    "x_C6 = torch.ones((6, 1))  # initial features all identical\n",
    "data_C6 = Data(x=x_C6, edge_index=edge_index_C6)\n",
    "\n",
    "# Two triangles (C3 + C3)\n",
    "edge_index_C3C3 = torch.tensor([\n",
    "    [0,1,2, 3,4,5, 1,2,0, 4,5,3],\n",
    "    [1,2,0, 4,5,3, 0,1,2, 3,4,5]\n",
    "], dtype=torch.long)\n",
    "x_C3C3 = torch.ones((6, 1))  # same shape as C6\n",
    "data_C3C3 = Data(x=x_C3C3, edge_index=edge_index_C3C3)\n",
    "\n",
    "# run the model\n",
    "model = SimpleGCN()\n",
    "with torch.no_grad():\n",
    "    out_C6 = model(data_C6.x, data_C6.edge_index, data_C6.batch)\n",
    "    out_C3C3 = model(data_C3C3.x, data_C3C3.edge_index, data_C3C3.batch)\n",
    "\n",
    "print(\"C6:\", out_C6)\n",
    "print(\"C3C3:\", out_C3C3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e03a6b",
   "metadata": {},
   "source": [
    "5. Interpret your findings about how good are message-passing based GNNs on counting cycle lengths. What's the effect of global sum pooling and does sum pooling solve the issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f522b4b",
   "metadata": {},
   "source": [
    "## Positional Awareness in GNNs\n",
    "\n",
    "Consider the following path graph $G$ where all node features are zero.\n",
    "\n",
    "![alt text](assets/w7_anchor.png \"G\")\n",
    "\n",
    "Answer briefly:\n",
    "\n",
    "1. Can any message passing GNN distinguish nodes 2 and 5, why? (Hint: You can compare their computation graphs.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8202edc",
   "metadata": {},
   "source": [
    "2. Now, let's pick node 1 as an *anchor node*. We do this by defining a new feature for each node as follows:\n",
    "    $$\n",
    "        d_1(v) = \\text{dist}(v, \\text{anchor node }1)\n",
    "    $$\n",
    "\n",
    "    Compute new (initial) node features that include $d_1(v)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e82247",
   "metadata": {},
   "source": [
    "3. Can nodes 2 and 5 be identified now? What about 3 and 4?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1f4cb",
   "metadata": {},
   "source": [
    "4. How many anchors do we need and which nodes to select as anchors to distinguish all nodes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4471d4",
   "metadata": {},
   "source": [
    "## When We Really Want to Count Cycles\n",
    "\n",
    "Let's revisit the cycle counting problem. A recent paper from ICLR 2024 ([link](https://openreview.net/pdf?id=qaJxPhkYtD)) proposes `Moment-GNN` and shows that if we feed random node features (IDs) and use a simple sumâ€‘aggregation GNN + a polynomial nonlinearity (\"moment\" layer), then the resulting graph embedding encodes statistics that correlate with substructure counts (e.g., counts of closed walks, which relate to cycles).\n",
    "\n",
    "We'll test the following graphs:\n",
    "\n",
    "![alt text](assets/w7_cycles_3.png \"G\")\n",
    "\n",
    "In the following code snippet, we reuse the graphs definitions we used so far. Also, we have a simplified `MomentGNN` implementation.\n",
    "\n",
    "Notice that we still have the idential initial node features for all graphs.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Run the `MomentGNN` with identical initial node features and interpret the output. Can it distinguish the graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import scatter\n",
    "\n",
    "# C3\n",
    "edge_index_C3 = torch.tensor([\n",
    "    [0,1, 1,2, 2,0],\n",
    "    [1,0, 2,1, 0,2]\n",
    "], dtype=torch.long)\n",
    "x_C3 = torch.ones((3, 1))  # initial features all identical\n",
    "data_C3 = Data(x=x_C3, edge_index=edge_index_C3)\n",
    "\n",
    "# C4\n",
    "edge_index_C4 = torch.tensor([\n",
    "    [0,1, 1,2, 2,3, 3,0],\n",
    "    [1,0, 2,1, 3,2, 0,3]\n",
    "], dtype=torch.long)\n",
    "x_C4 = torch.ones((4, 1))  # initial features all identical\n",
    "data_C4 = Data(x=x_C4, edge_index=edge_index_C4)\n",
    "\n",
    "# C6\n",
    "edge_index_C6 = torch.tensor([\n",
    "    [0, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 0],\n",
    "    [1, 2, 3, 4, 5, 0, 0, 1, 2, 3, 4, 5]\n",
    "], dtype=torch.long)\n",
    "x_C6 = torch.ones((6, 1))  # initial features all identical\n",
    "data_C6 = Data(x=x_C6, edge_index=edge_index_C6)\n",
    "\n",
    "# Two triangles (C3 + C3)\n",
    "edge_index_C3C3 = torch.tensor([\n",
    "    [0,1,2, 3,4,5, 1,2,0, 4,5,3],\n",
    "    [1,2,0, 4,5,3, 0,1,2, 3,4,5]\n",
    "], dtype=torch.long)\n",
    "x_C3C3 = torch.ones((6, 1))  # initial features all identical\n",
    "data_C3C3 = Data(x=x_C3C3, edge_index=edge_index_C3C3)\n",
    "\n",
    "def moment_gnn_stat(data):\n",
    "    x = data.x\n",
    "    edge_index = data.edge_index\n",
    "\n",
    "    # aggregate incoming messages from neighbors using sum\n",
    "    agg = scatter(\n",
    "        x[edge_index[0]], # source node features\n",
    "        edge_index[1], # index of target nodes\n",
    "        dim=0,\n",
    "        dim_size=x.size(0),\n",
    "        reduce='sum'\n",
    "    )\n",
    "\n",
    "    # add the aggregated messages to the original features\n",
    "    h1 = x + agg\n",
    "\n",
    "    # element-wise square\n",
    "    h2 = h1 * h1\n",
    "\n",
    "    # sum pooling\n",
    "    g = h2.sum(dim=0)\n",
    "\n",
    "    # return the L2 norm of the graph representation\n",
    "    return torch.norm(g).item()\n",
    "\n",
    "# Run experiments\n",
    "graphs = {\n",
    "    \"C3\": data_C3,\n",
    "    \"C4\": data_C4,\n",
    "    \"two_C3\": data_C3C3,\n",
    "    \"C6\": data_C6\n",
    "}\n",
    "\n",
    "for name, G in graphs.items():\n",
    "    print(name, moment_gnn_stat(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80f855",
   "metadata": {},
   "source": [
    "2. Edit the code snippet so node features are random $d$-dimensional vectors. Try different values of $d$ and interpret your results. Are the final embeddings of all graphs different?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
