{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02dc8ff0",
   "metadata": {},
   "source": [
    "# Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694beee1",
   "metadata": {},
   "source": [
    "## Warm-up: What is actually learned?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564b6888",
   "metadata": {},
   "source": [
    "In KG embedding models like TransE:\n",
    "\n",
    "1. What are the trainable parameters?\n",
    "\n",
    "1. Suppose a new entity $e_{\\text{new}}$ is added to the KG after training. Can TransE produce an embedding for it without retraining? Why or why not?\n",
    "\n",
    "1. Let's assume that you observe one triple $(h, r, e_{\\text{new}})$. Could you heuristically assign an embedding to $e_{\\text{new}}$? What are the limitations? Hint: Think of the translation $\\mathbf{h} + \\mathbf{r} \\approx \\mathbf{t}$.\n",
    "\n",
    "1. Given a trained TransE model and a query $(h, r, ?)$, inference is performed by computing the query embedding $\\mathbf q = \\mathbf h + \\mathbf r$. How can we get semantic understanding of what the embedding vector $\\mathbf{q}$ corresponds to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e140e62",
   "metadata": {},
   "source": [
    "## TransE Mechanics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0696bc33",
   "metadata": {},
   "source": [
    "Consider the following 2D embeddings:\n",
    "\n",
    "- $\\mathbf{h} = (1, 0)$\n",
    "- $\\mathbf{r} = (1, 1)$\n",
    "- $\\mathbf{t_1} = (2, 1)$\n",
    "- $\\mathbf{t_2} = (2, 2)$\n",
    "\n",
    "1. Compute the TransE score $f_r(h,t) = -\\lVert h + r - t \\rVert_2$ for $t_1$ and $t_2$. Which triple is more plausible?\n",
    "\n",
    "1. Assume that relation $r$ is symmetric (e.g., \"siblingOf\"), meaning: $(h,r,t_1)$ and $(t_1,r,h)$ are both true. Write down the TransE equations implied by symmetry. What's the issue here?\n",
    "\n",
    "1. Assume that relation $r$ is 1-to-N (e.g., \"studentOf\"): $(h,r,t_1)$ and $(h,r,t_2)$ are true with $t_1 \\neq t_2$. Write the TransE equations for both triples. What geometric constraint does this impose on $t_1$ and $t_2$?\n",
    "\n",
    "1. Suppose we increase the embedding dimension from $k=2$ to $k=100$. Does this resolve the issue in (2) and (3)? Justify mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39553a68",
   "metadata": {},
   "source": [
    "## Path Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853336f9",
   "metadata": {},
   "source": [
    "Consider a knowledge graph with the following entity types:\n",
    "\n",
    "- Person\n",
    "- Company\n",
    "- City\n",
    "- University\n",
    "- Country\n",
    "\n",
    "and the following directed relations:\n",
    "\n",
    "- worksAt(Person → Company)\n",
    "- locatedIn(Company → City)\n",
    "- studiedAt(Person → University)\n",
    "- locatedIn(University → City)\n",
    "- basedIn(City → Country)\n",
    "- foundedBy(Company → Person)\n",
    "\n",
    "1. For each of the following natural-language queries, write the formal path query.\n",
    "\n",
    "    - $Q_1$: Which company does Alice work at?\n",
    "    - $Q_2$: In which city is the company where Alice works located?\n",
    "    - $Q_3$: In which country is the company where Alice works based?\n",
    "    - $Q_4$: Which people work at companies located in Berlin?\n",
    "    - $Q_5$: Which people studied at universities located in the same city as the company where Alice works?\n",
    "    - $Q_6$: Which people studied at universities located in cities where companies founded by Bob are based?\n",
    "    - $Q_7$: Which people both studied at universities in Berlin and work at companies based in Germany?\n",
    "\n",
    "1. Which of the queries in (1) can TransE represent?\n",
    "\n",
    "1. What if we simply use $-r$ to represent inverse relations. Would that work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218dd2d5",
   "metadata": {},
   "source": [
    "## Programming: TransE on a Toy KG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d3ad0",
   "metadata": {},
   "source": [
    "In this exercise, we'll use the `PyG` implementation of `TransE` on a toy KG, which is given in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdf2e934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn.kge import TransE\n",
    "\n",
    "# toy KG\n",
    "entities = [\n",
    "    \"Alice\", \"Bob\", \"Charlie\",\n",
    "    \"CompanyA\", \"CompanyB\",\n",
    "    \"Berlin\", \"Munich\"\n",
    "]\n",
    "relations = [\"worksAt\", \"locatedIn\"]\n",
    "triples = [\n",
    "    (\"Alice\",   \"worksAt\",   \"CompanyA\"),\n",
    "    (\"Bob\",     \"worksAt\",   \"CompanyA\"),\n",
    "    (\"Charlie\", \"worksAt\",   \"CompanyB\"),\n",
    "    (\"CompanyA\",\"locatedIn\", \"Berlin\"),\n",
    "    (\"CompanyB\",\"locatedIn\", \"Munich\"),\n",
    "]\n",
    "\n",
    "# helper mappings\n",
    "ent2id = {e: i for i, e in enumerate(entities)}\n",
    "rel2id = {r: i for i, r in enumerate(relations)}\n",
    "\n",
    "head = torch.tensor([ent2id[h] for h, _, _ in triples], dtype=torch.long)\n",
    "rel  = torch.tensor([rel2id[r] for _, r, _ in triples], dtype=torch.long)\n",
    "tail = torch.tensor([ent2id[t] for _, _, t in triples], dtype=torch.long)\n",
    "\n",
    "# store as PyG data\n",
    "edge_index = torch.stack([head, tail], dim=0)\n",
    "edge_type = rel\n",
    "data = Data(num_nodes=len(entities), edge_index=edge_index, edge_type=edge_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285d7f4",
   "metadata": {},
   "source": [
    "In the following code snippet, we define the model using the `TransE` implementation in `PyG`. Complete the `train` function. Use all the triples given in the KG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d081c0",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "# model setup\n",
    "model = TransE(\n",
    "    num_nodes=data.num_nodes,\n",
    "    num_relations=len(relations),\n",
    "    hidden_channels=2,\n",
    "    margin=1.0,\n",
    "    p_norm=2.0, # use L2 distance\n",
    "    sparse=False,\n",
    ")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# training loop\n",
    "def train(steps=2000, seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    model.train()\n",
    "\n",
    "    # === YOUR CODE HERE ===\n",
    "\n",
    "    # ======================\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a029ad",
   "metadata": {},
   "source": [
    "Now, using the following inference functions, test the given cases:\n",
    "\n",
    "- Rank Company tails for (Alice, worksAt, ?)\n",
    "- Rank City tails for (CompanyA, locatedIn, ?)\n",
    "- Vector-space path query: q = Alice + worksAt + locatedIn, rank cities by distance\n",
    "- Heuristic inverse: q = CompanyA - worksAt, rank persons by distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ad13052",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "# inference helpers\n",
    "@torch.no_grad()\n",
    "def rank_tails(head_name: str, rel_name: str, candidate_tail_names=None, topk=5):\n",
    "    \"\"\"\n",
    "    Ranks candidate tails for query (head, rel, ?) by TransE score.\n",
    "    Higher score = more plausible.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    h = torch.tensor([ent2id[head_name]], dtype=torch.long)\n",
    "    r = torch.tensor([rel2id[rel_name]], dtype=torch.long)\n",
    "\n",
    "    if candidate_tail_names is None:\n",
    "        cand_ids = torch.arange(len(entities), dtype=torch.long)\n",
    "        cand_names = entities\n",
    "    else:\n",
    "        cand_ids = torch.tensor([ent2id[x] for x in candidate_tail_names], dtype=torch.long)\n",
    "        cand_names = candidate_tail_names\n",
    "\n",
    "    # broadcast h,r to match candidates\n",
    "    h_rep = h.repeat(cand_ids.numel())\n",
    "    r_rep = r.repeat(cand_ids.numel())\n",
    "\n",
    "    scores = model(h_rep, r_rep, cand_ids)  # forward() returns score for triplets\n",
    "    vals, pos = torch.topk(scores, k=min(topk, scores.numel()), largest=True)\n",
    "\n",
    "    return [(cand_names[i], float(v)) for i, v in zip(pos.tolist(), vals.tolist())]\n",
    "\n",
    "# for vector-space traversal (q = h + r1 + r2)\n",
    "@torch.no_grad()\n",
    "def get_entity_emb(name: str) -> torch.Tensor:\n",
    "    return model.node_emb.weight[ent2id[name]]\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_rel_emb(name: str) -> torch.Tensor:\n",
    "    return model.rel_emb.weight[rel2id[name]]\n",
    "\n",
    "@torch.no_grad()\n",
    "def rank_by_query_vector(q_vec: torch.Tensor, candidate_names):\n",
    "    cand = torch.stack([get_entity_emb(n) for n in candidate_names], dim=0)\n",
    "    # TransE score is -||q - t||, so ranking by smallest distance is equivalent\n",
    "    d = torch.norm(cand - q_vec.unsqueeze(0), p=2, dim=-1)\n",
    "    order = torch.argsort(d, descending=False)\n",
    "    return [(candidate_names[i], float(d[i])) for i in order.tolist()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
