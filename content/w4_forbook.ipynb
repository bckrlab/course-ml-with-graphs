{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aeddea7",
   "metadata": {},
   "source": [
    "# A General Perspective on Graph Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be0b8b6",
   "metadata": {},
   "source": [
    "## General GNN Framework\n",
    "\n",
    "Consider the following GNN layer:\n",
    "\n",
    "$$\n",
    "    h_v^{(l)} = \\sigma \\left( \\operatorname{AGG}^{(l)} \\left( \\{\\operatorname{MSG}^{(l)}(h_u^{(l-1)} \\mid u \\in N(v))\\} \\right) \\right)\n",
    "$$\n",
    "\n",
    "1. Explain the role of `AGG` and `MSG` components.\n",
    "1. Why must `AGG` be permutation invariant?\n",
    "1. What issue arises in this formulation? (Hint: Consider how node $v$ incorporates information about itself.)\n",
    "1. How can we modify this equation to fix the issue in (3)?\n",
    "1. Modify the layer so that self-information and neighbor-information are treated differently. Provide the updated equation. (Hint: You are not limited to a single trainable weight matrix.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f541fe",
   "metadata": {},
   "source": [
    "## Depth of a GNN\n",
    "\n",
    "Consider the following GNN layer:\n",
    "$$\n",
    "    h_v^{(l)} = \\operatorname{CONCAT} \\big( \\operatorname{AGG} \\big( \\{ m_u^{(l)} \\mid u \\in N(v) \\} \\big), m_v^{(l)} \\big)\n",
    "$$\n",
    "where $m_u^{(l)}=\\operatorname{MSG}^{(l)}(h_u^{(l-1)})$ are the messages from neighbors and $m_v^{(l)}=\\operatorname{MSG}^{(l)}(h_v^{(l-1)})$ is the self-message.\n",
    "\n",
    "1. Assume that the `AGG` is the `SUM` function, and the messages $m_u^{(l)}$ and $m_v^{(l)}$ are $d$-dimensional vectors. What is the dimension of $h_v^{(l)}$?\n",
    "1. Discuss potential issues or challenges this concatenation might introduce when stacking multiple GNN layers.\n",
    "1. Suggest a technique to control the dimension after concatenation.\n",
    "1. Now, assume that we change the layer definition so that the concatenated vector is fed to a $k$-layer multi-layer perceptron (MLP) as given in the following:\n",
    "    $$\n",
    "        h_v^{(l)} = \\operatorname{MLP} \\big( \\operatorname{CONCAT} \\big( \\operatorname{AGG} \\big( \\{ m_u^{(l)} \\mid u \\in N(v) \\} \\big), m_v^{(l)} \\big) \\big)\n",
    "    $$\n",
    "    If we stack $L$ such layers, what will be the total depth of the GNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1207d4",
   "metadata": {},
   "source": [
    "## Graph Convolutional Networks\n",
    "\n",
    "Consider the layer definition of the GCN by Kipf & Welling (paper [link](https://arxiv.org/abs/1609.02907)):\n",
    "\n",
    "$$\n",
    "    h_v^{(l+1)} = \\sigma\\left( \\sum_{u \\in N(v) \\cup \\{v\\}} \\frac{1}{\\sqrt{\\tilde{d}_v \\tilde{d}_u}} \\, h_u^{(l)} W^{(l)} \\right)\n",
    "$$\n",
    "where $\\tilde{d}_v$ is the degree of node $v$ in the augmented adjacency $\\tilde{A} = A + I$ after adding self-loops.\n",
    "\n",
    "1. Compared to the simple normalization factor $\\frac{1}{|N(v)|}$ we have seen in the lecture (W4, slide 17), what could be the reason to have $\\frac{1}{\\sqrt{\\tilde{d}_v \\tilde{d}_u}}$ (symmetric normalization)?\n",
    "\n",
    "1. How does the GCN process a node's own message differently from its neighbors' messages?\n",
    "\n",
    "1. GraphSAGE (SAmple and aggreGatE) by Hamilton et. al. (paper [link](https://arxiv.org/abs/1706.02216)) is an extension of the GCN framework. Given the pseudocode below, list three aspects of GraphSAGE that is different from the original GCN and discuss how these changes improve the method.\n",
    "\n",
    "    ![alt text](assets/w4_graphsage.png \"GraphSAGE Algorithm\")\n",
    "\n",
    "1. How does the Graph Attention Network by Veličković et. al. ([paper](https://arxiv.org/abs/1710.10903)) improve the GCN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581d3e9c",
   "metadata": {},
   "source": [
    "## Programming: Simple GNN with Torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf46d0d",
   "metadata": {},
   "source": [
    "In this exercise, you'll implement a simple GNN with `pytorch` and test on the `Cora` dataset ([link](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html)).\n",
    "\n",
    "The layer definition of the GNN you'll implement is given as follows:\n",
    "\n",
    "$$\n",
    "    h_v^{(l)} = \\sigma \\left( W_{\\text{self}}^{(l)} \\cdot h_v^{(l-1)} + W_{\\text{neigh}}^{(l)} \\cdot \\operatorname{AGG}^{(l)}\\left(\\{h_u^{(l-1)} \\mid u \\in N(v)\\}\\right) \\right)\n",
    "$$\n",
    "\n",
    "Basically, it uses two learnable weight matrices $W_{\\text{self}}$ and $W_{\\text{neigh}}$ that are multiplied with the messages from the node's self and aggregated messages from neighbors, respectively. Then, the resulting vectors are summed and fed to $\\sigma$.\n",
    "\n",
    "1. Complete the following code snippet given the following:\n",
    "    - Use `MEAN` aggretagor for neighbor messages.\n",
    "    - Use ReLU as $\\sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86c3f0",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# A simple GNN layer that\n",
    "# - aggregates neighbor features by mean\n",
    "# - uses separate weight matrices for self and neighbor features\n",
    "class SimpleGNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear_self = nn.Linear(in_dim, out_dim, bias=True)\n",
    "        self.linear_neigh = nn.Linear(in_dim, out_dim, bias=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "\n",
    "        # edges j -> i (col = source, row = target)\n",
    "        row, col = edge_index\n",
    "\n",
    "        # messages are neighbor features\n",
    "        messages = x[col]\n",
    "\n",
    "        # aggregate by mean\n",
    "        ############# your code here ############\n",
    "\n",
    "\n",
    "    \n",
    "        #########################################\n",
    "\n",
    "        # update rule: combine self and neighbors with separate weights\n",
    "        ############# your code here ############\n",
    "\n",
    "\n",
    "    \n",
    "        #########################################\n",
    "\n",
    "        # return updated node features\n",
    "        return out\n",
    "\n",
    "# A simple 2-layer GNN model\n",
    "class SimpleGNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = SimpleGNNLayer(in_dim, hidden_dim)\n",
    "        self.layer2 = SimpleGNNLayer(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        # apply GNN layers with ReLU non-linearity\n",
    "        ############# your code here ############\n",
    "\n",
    "\n",
    "    \n",
    "        #########################################\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baaab74",
   "metadata": {},
   "source": [
    "Now, train your GNN using the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cca845",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "\n",
    "# load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/Cora', name='Cora')\n",
    "\n",
    "# model, data, optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleGNN(in_dim=dataset.num_node_features, hidden_dim=16, out_dim=dataset.num_classes).to(device)\n",
    "data = dataset[0].to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "\n",
    "# training loop\n",
    "train_losses = []\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    \n",
    "    # training loss\n",
    "    loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    print(f\"Epoch {epoch}: Train Loss = {loss.item():.4f}\")\n",
    "\n",
    "# final eval\n",
    "model.eval()\n",
    "\n",
    "pred = model(data).argmax(dim=1)\n",
    "correct = (pred[data.test_mask] == data.y[data.test_mask]).sum()\n",
    "acc = int(correct) / int(data.test_mask.sum())\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298d6f76",
   "metadata": {},
   "source": [
    "2. Now, modify the `SimpleGNN` class so that it takes the number of layers as input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b33fa74",
   "metadata": {},
   "source": [
    "3. Now, train two GNNs, one with 2 layers and another with 16. Then, compare the Mean Average Distance (MAD) of the embeddings learned by the models using the given function.\n",
    "\n",
    "    - Mean Average Distance (MAD): Average pairwise Euclidean distance between node embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffc5b255",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "def get_mad(embeddings):\n",
    "\n",
    "    # embeddings: [N, d]  (N nodes, d-dimensional embeddings)\n",
    "    N = embeddings.size(0)\n",
    "\n",
    "    # Normalize embeddings\n",
    "    embeddings = F.normalize(embeddings, dim=1)\n",
    "\n",
    "    # Compute all pairwise Euclidean distances → shape [N, N]\n",
    "    dist_matrix = torch.cdist(embeddings, embeddings, p=2)\n",
    "\n",
    "    # Average the distances over all distinct pairs\n",
    "    mad_val = dist_matrix.sum() / (N * (N - 1))\n",
    "\n",
    "    return mad_val.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ac472",
   "metadata": {},
   "source": [
    "4. Interpret the accuracy and MAD values of models with 2 and 16 layers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
