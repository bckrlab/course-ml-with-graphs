{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1b4634d",
   "metadata": {},
   "source": [
    "# Theory of GNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fd68ab",
   "metadata": {},
   "source": [
    "## Graph Isomorphism: It’s Harder Than It Looks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f69709",
   "metadata": {},
   "source": [
    "**Definition:** Two graphs $G$ and $H$ are **isomorphic** (iso: equal/same, morph: form/shape) if there exists a matching between their vertices so that two vertices are connected by an edge in $H$ if and only if corresponding vertices are connected by an edge in $G$.\n",
    "\n",
    "Optional Warm-Up: Play the following game for a few rounds: [Isomorphism Game](https://github.ericmickelsen.com/graf/isomorphism.html) (Credit: Eric Mickelsen). Hint: You can move the nodes around.\n",
    "\n",
    "Consider the following graphs $G$ and $H$:\n",
    "\n",
    "![alt text](assets/w6_gh.png \"GH\"){width=400px}\n",
    "\n",
    "1. You are asked to check if these graphs are isomorphic or not. Find a mapping from nodes of $G$ to nodes of $H$ so that the structure is preserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c337ecc8",
   "metadata": {},
   "source": [
    "2. At first glance, graph isomorphism may look simple: just \"match the nodes\". However, as you can see, even for small graphs, finding an isomorphic mapping is not straightforward. For reference, in a protein-protein interaction (PPI) graph of the \"simple\" Escherichia Coli K12 bacterium, there are $\\approx 4100$ nodes and $\\approx 500000$ edges.\n",
    "\n",
    "    Now let's think of the most naive approach to check if two given graphs are isomorphic. How many different matchings we need to check between two $n$-node graphs in the worst case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f99ae4",
   "metadata": {},
   "source": [
    "3. Let's run a very simple GNN on these graphs:\n",
    "\n",
    "    $$\n",
    "        h_v^{(l)} = \\sigma \\left( W^{(l)} \\sum_{u \\in N(v)} \\frac{h_u^{(l-1)}}{|N(v)|} \\right)\n",
    "    $$\n",
    "\n",
    "    where $W=I$ (identity matrix), $\\sigma$ is ReLU and all the nodes have the same initial feature vector $(1,1)^\\top$.\n",
    "\n",
    "    Compute the embeddings of node $1$ in graph $G$ and node $s$ in graph $H$ after one iteration of message passing. Are they the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e99f5e",
   "metadata": {},
   "source": [
    "4. Based on (3) and the structure of both graphs, infer the graph-level embeddings of $G$ and $H$ obtained with mean-pooling (i.e., we take the average of all node embeddings in the graph). Are they the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e1791e",
   "metadata": {},
   "source": [
    "5. Same graph-level embeddings mean that the GNN we used cannot distinguish isomorphic graphs. Is this a problem or a desirable property?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87f830",
   "metadata": {},
   "source": [
    "## The Weisfeiler-Lehman Isomorphism Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316c63d",
   "metadata": {},
   "source": [
    "Luckily, brute-force checking of node labelings is not the only way to test if two graphs are isomorphic or not. Although it's not perfect, the Weisfeiler-Lehman (WL) test is an efficient heuristic.\n",
    "\n",
    "- Initialization: Assign all nodes an initial label (e.g., 1).\n",
    "- Iteration: For each node $v$:\n",
    "    1. Collect labels of all neighbors.\n",
    "    2. Form a multiset with its own label.\n",
    "    3. Hash this multiset into a new label for the node\n",
    "- Check convergence: Stop if labels don’t change, otherwise repeat.\n",
    "- Compare graphs: Collect the multiset of final node labels for each graph. If the multisets differ, then graphs are non-isomorphic. If identical → 1-WL cannot distinguish them.\n",
    "\n",
    "Consider the following graphs $G$ and $H$:\n",
    "\n",
    "![alt text](assets/w6_gh2.png \"GH2\"){width=400px}\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Apply WL test to determine if $G$ and $H$ are isomorphic or not. You can check this [link](https://davidbieber.com/post/2019-05-10-weisfeiler-lehman-isomorphism-test/) for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab6564",
   "metadata": {},
   "source": [
    "## Multisets and Injectivity\n",
    "\n",
    "Consider the following multisets:\n",
    "\n",
    "- $M_1 = \\{ 1,3 \\}$\n",
    "- $M_2 = \\{ 1,1,3,3 \\}$\n",
    "- $M_3 = \\{ 1,1,1,1,1 \\}$\n",
    "- $M_4 = \\{ 2,3 \\}$\n",
    "\n",
    "1. Show that none of `MEAN`, `MAX` and `SUM` is injective over these multisets.\n",
    "\n",
    "2. What about $\\sum_{m\\in M}2^m$?\n",
    "\n",
    "3. What about $\\sum_{m\\in M}e^m$?\n",
    "\n",
    "4. What's the reason that $\\sum_{m\\in M}2^m$ produces collisions but $\\sum_{m\\in M}e^m$ does not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c1a7d8",
   "metadata": {},
   "source": [
    "## Programming: Expressivity of GNNs, GCN vs GraphSAGE vs GIN\n",
    "\n",
    "In this exercise, you will empirically investigate the expressive power of three Graph Neural Networks:\n",
    "\n",
    "- GCN (Kipf & Welling)\n",
    "- GraphSAGE (mean aggregator)\n",
    "- GIN (Graph Isomorphism Network)\n",
    "\n",
    "You will compare how these architectures process two small graphs and analyze whether they can distinguish different structural roles of nodes. The graphs $G$ and $H$ are given in the following.\n",
    "\n",
    "![alt text](assets/w6_gh3.png \"GH3\"){width=400px}\n",
    "\n",
    "We assume that all nodes have the same feature vector $[1]$.\n",
    "\n",
    "First, let's construct $G$ and $H$ in `PyG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "afe03830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Graph G: 3-node path (1 - 0 - 2)\n",
    "g_edge_index = torch.tensor([\n",
    "    [0,1,0,2],\n",
    "    [1,0,2,0]\n",
    "], dtype=torch.long)\n",
    "g_x = torch.tensor([[1], [1], [1]], dtype=torch.float) # all features = 1\n",
    "data_g = Data(x=g_x, edge_index=g_edge_index)\n",
    "\n",
    "# Graph H: diamond graph\n",
    "h_edge_index = torch.tensor([\n",
    "    [0,1,0,2,0,3,0,4],\n",
    "    [1,0,2,0,3,0,4,0]\n",
    "], dtype=torch.long)\n",
    "h_x = torch.tensor([[1], [1], [1], [1], [1]], dtype=torch.float) # all features = 1\n",
    "data_h = Data(x=h_x, edge_index=h_edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ad8f7",
   "metadata": {},
   "source": [
    "Next, we'll use the following implementations for `GCN`, `GraphSAGE` and `GIN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44854f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GINConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels, aggr='mean')\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels, aggr='mean')\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        nn1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(in_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels)\n",
    "        )\n",
    "        nn2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(hidden_channels, hidden_channels),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(hidden_channels, out_channels)\n",
    "        )\n",
    "        self.conv1 = GINConv(nn1)\n",
    "        self.conv2 = GINConv(nn2)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a1b91e",
   "metadata": {},
   "source": [
    "Now, let's run one forward pass to see how the node embeddings change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a75b379",
   "metadata": {
    "tags": [
     "q_code"
    ]
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"GCN\": GCN(1, 16, 2),\n",
    "    \"GraphSAGE\": GraphSAGE(1, 16, 2),\n",
    "    \"GIN\": GIN(1, 16, 2)\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.eval()\n",
    "    print(f\"=== {name} ===\")\n",
    "    for label, data in zip([\"G\", \"H\"], [data_g, data_h]):\n",
    "        with torch.no_grad():\n",
    "            emb = model(data.x, data.edge_index)\n",
    "        print(f\"{label} node embeddings:\\n{emb}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6f8e7",
   "metadata": {},
   "source": [
    "Task: Analyze the results:\n",
    "\n",
    "1. Which models distinguish the center node (0) from the leaves (1 and 2)?\n",
    "\n",
    "1. Compare the embeddings of node 0 in graphs $G$ and $H$. Which models produce noticeably different embeddings?\n",
    "\n",
    "1. Why GraphSAGE with mean aggregation collapses all embeddings when features start identical? Would it change if we switch to max aggregation as given below?\n",
    "\n",
    "    `SAGEConv(in_channels, hidden_channels, aggr='max')`\n",
    "\n",
    "1. GCN also uses mean aggregator, but it can distinguish the center node, how?\n",
    "\n",
    "1. To distinguish $G$ and $H$ with `GraphSAGE`, which global pooling method would you use, why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
